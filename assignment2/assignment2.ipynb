{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST hand-written digit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit range from 0~9, training set consist of 60000 images, and test set consist of 10000 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: One layer SoftMax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forward Pass of SoftMax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/formula1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path='/train', test_path='test/'):\n",
    "    train_list = glob.glob(os.path.join(train_path, '*.png'))\n",
    "    pattern = re.compile(r'num(\\d).png')\n",
    "    train_id = np.array([float(pattern.search(img_name).groups()[0]) for img_name in train_list])\n",
    "    train_data = np.concatenate([np.array(Image.open(img_name)).reshape((1, 784))for img_name in tqdm_notebook(train_list, leave=False)],\n",
    "                                axis=0).astype(np.float)\n",
    "\n",
    "    test_list = glob.glob(os.path.join(test_path, '*.png'))\n",
    "    test_id = np.array([float(pattern.search(img_name).groups()[0]) for img_name in test_list])\n",
    "    test_data = np.concatenate([np.array(Image.open(img_name)).reshape((1, 784)) for img_name in tqdm_notebook(test_list, leave=False)],\n",
    "                               axis=0).astype(np.float)\n",
    "\n",
    "    return train_data, train_id, test_data, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'C:\\Users\\GS65\\Desktop\\assignment2\\train' \n",
    "test_path = r'C:\\Users\\GS65\\Desktop\\assignment2\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_data, train_id, test_data, test_id = load_data(train_path,test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    exp_scores = np.nan_to_num(np.exp(scores))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims = True)\n",
    "    probs = np.nan_to_num(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_compute(probs, labels, reg=0, W=0):\n",
    "    L = probs[range(probs.shape[0]),labels.astype('int64')]\n",
    "    L = np.nan_to_num(-np.log(L))\n",
    "    data_loss = np.sum(L)/L.shape[0]\n",
    "    if reg:\n",
    "        reg_loss = 0.5 * reg * np.sum(W * W)\n",
    "        return (data_loss + reg_loss)\n",
    "    else:\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_layer_classifier(x_train, y_train, epochs, mini_batch_size, step, reg=0):\n",
    "    np.random.seed(42)\n",
    "    w = 0.01 * np.random.randn(x_train.shape[1],10)\n",
    "    b = np.zeros((1,10))\n",
    "    training = [(x,y) for x,y in zip(x_train,y_train)]\n",
    "    n = len(x_train)\n",
    "    for i in range(epochs):\n",
    "        random.shuffle(training)\n",
    "        mini_batches = [training[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "    \n",
    "        for mini_batch in mini_batches:\n",
    "            # forward\n",
    "            x_batch = np.concatenate([np.array(t[0]).reshape((1, 784)) for t in mini_batch])\n",
    "            y_batch = np.array([t[1] for t in mini_batch])\n",
    "            scores = np.dot(x_batch, w) + b\n",
    "            probs = softmax(scores)\n",
    "            # backward\n",
    "            dscores = probs\n",
    "            dscores[range(x_batch.shape[0]),y_batch.astype('int64')] -= 1\n",
    "            dscores = dscores/x_batch.shape[0]\n",
    "            dw = np.dot(x_batch.T, dscores)\n",
    "            dw += reg * w\n",
    "            db = np.sum(dscores, axis=0, keepdims=True)\n",
    "            w += -step * dw/len(mini_batch)\n",
    "            b += -step * db/len(mini_batch)\n",
    "\n",
    "\n",
    "        # compute loss      \n",
    "        if i % 100 ==0:\n",
    "            t_scores = np.dot(x_train, w) + b\n",
    "            t_probs = softmax(t_scores)\n",
    "            loss = loss_compute(t_probs,y_train,reg,w)\n",
    "            print (\"iteration %d: loss %f\" % (i, loss))       \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w, b, x_test, y_test):\n",
    "    scores = np.dot(x_test,w) + b\n",
    "    predict = np.argmax(scores, axis=1)\n",
    "    print('training accuracy: %.2f' % (np.mean(predict == y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.291193\n",
      "iteration 100: loss 0.481483\n",
      "iteration 200: loss 0.458341\n",
      "iteration 300: loss 0.361681\n",
      "iteration 400: loss 0.545539\n",
      "iteration 500: loss 0.387182\n",
      "iteration 600: loss 0.450068\n",
      "iteration 700: loss 0.361206\n",
      "iteration 800: loss 0.406088\n",
      "iteration 900: loss 0.516441\n",
      "training accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "w, b = One_layer_classifier(train_data, train_id, 1000, 100, 0.005, reg=0)\n",
    "accuracy(w, b, test_data, test_id)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.312432\n",
      "iteration 100: loss 0.577875\n",
      "iteration 200: loss 0.490529\n",
      "iteration 300: loss 0.400090\n",
      "iteration 400: loss 0.602941\n",
      "iteration 500: loss 0.476072\n",
      "iteration 600: loss 0.523681\n",
      "iteration 700: loss 0.440059\n",
      "iteration 800: loss 0.528997\n",
      "iteration 900: loss 0.397138\n",
      "training accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "w, b = One_layer_classifier(train_data, train_id, 1000, 100, 0.005, reg=0.5)\n",
    "accuracy(w, b, test_data, test_id)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Two Layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forward Pass of a Two layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/formula2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self,sizes):\n",
    "        self.sizes = sizes\n",
    "        self.b = [0.01 * np.random.randn(1, y) for y in sizes[1:]]\n",
    "        self.w = [0.01 * np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def loss_compute(self, x_train, labels, reg=0):\n",
    "        hidden_layer = np.maximum(0, np.dot(x_train, self.w[0]) + self.b[0])\n",
    "        scores = np.dot(hidden_layer, self.w[1]) + self.b[1]\n",
    "        probs = softmax(scores)\n",
    "        L = probs[range(probs.shape[0]),labels.astype('int64')]\n",
    "        L = np.nan_to_num(-np.log(L))\n",
    "        data_loss = np.sum(L)/L.shape[0]\n",
    "        if reg:\n",
    "            reg_loss1 = 0.5 * reg * np.sum(self.w[0] * self.w[0]) \n",
    "            reg_loss2 = 0.5 * reg * np.sum(self.w[1] * self.w[1])\n",
    "            reg_loss = reg_loss1 + reg_loss2\n",
    "            return (data_loss + reg_loss)\n",
    "        else:\n",
    "            return data_loss\n",
    "\n",
    "    def SGD(self, x_train, y_train, epochs, mini_batch_size, step, reg=0):\n",
    "        training = [(x,y) for x,y in zip(x_train,y_train)]\n",
    "        n = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training)\n",
    "            mini_batches = [training[k:k+mini_batch_size]\n",
    "                    for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                # forward\n",
    "                x_batch = np.concatenate([np.array(t[0]).reshape((1, 784)) for t in mini_batch])\n",
    "                y_batch = np.array([t[1] for t in mini_batch])\n",
    "                hidden_layer = np.maximum(0, np.dot(x_batch, self.w[0]) + self.b[0])\n",
    "                scores = np.dot(hidden_layer, self.w[1]) + self.b[1]\n",
    "                probs = softmax(scores)\n",
    "                # backward\n",
    "                dscores = probs\n",
    "                dscores[range(x_batch.shape[0]),y_batch.astype('int64')] -= 1\n",
    "                dscores /= x_batch.shape[0]\n",
    "                \n",
    "                dw2 = np.dot(hidden_layer.T, dscores)\n",
    "                db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "                dhidden = np.dot(dscores, (self.w[1]).T)\n",
    "                dhidden[hidden_layer <= 0] = 0\n",
    "                dw = np.dot(x_batch.T, dhidden)\n",
    "                db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "                dw2 += reg * self.w[1]\n",
    "                dw += reg * self.w[0]\n",
    "                \n",
    "                # update\n",
    "                m = len(mini_batch)\n",
    "                self.w[0] += -step * dw / m\n",
    "                self.b[0] += -step * db / m\n",
    "                self.w[1] += -step * dw2 / m\n",
    "                self.b[1] += -step * db2 / m\n",
    "\n",
    "         \n",
    "            # compute loss      \n",
    "            if i % 10 ==0:\n",
    "                loss = self.loss_compute(x_train,y_train,reg)\n",
    "                print (\"iteration %d: loss %f\" % (i, loss))       \n",
    "\n",
    "    def accuracy(self, x_test, y_test):\n",
    "        hidden_layer = np.maximum(0, np.dot(x_test, self.w[0]) + self.b[0])\n",
    "        scores = np.dot(hidden_layer,self.w[1] ) + self.b[1]\n",
    "        predict = np.argmax(scores, axis=1)\n",
    "        print('training accuracy: %.2f' % (np.mean(predict == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.297819\n",
      "iteration 10: loss 0.095195\n",
      "iteration 20: loss 0.098444\n",
      "iteration 30: loss 0.089829\n",
      "iteration 40: loss 0.088809\n",
      "iteration 50: loss 0.085916\n",
      "iteration 60: loss 0.080518\n",
      "iteration 70: loss 0.083492\n",
      "iteration 80: loss 0.092804\n",
      "iteration 90: loss 0.080891\n",
      "training accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "net = Network([784, 100, 10])\n",
    "net.SGD(train_data, train_id, 100, 30, 0.05, 0.05)\n",
    "net.accuracy(test_data, test_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
